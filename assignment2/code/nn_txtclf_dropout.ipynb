{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 1\n",
    "# Package imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "print(np.shape(categories))\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',  categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',  categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 742)\t0.04584677767961114\n",
      "  (0, 601)\t0.22178541984084713\n",
      "  (0, 88)\t0.1679900779387685\n",
      "  (0, 1879)\t0.15744988844762187\n",
      "  (0, 1746)\t0.02292338883980557\n",
      "  (0, 1986)\t0.06059476500051193\n",
      "  (0, 537)\t0.0679123091588565\n",
      "  (0, 135)\t0.08850994143992637\n",
      "  (0, 1801)\t0.19448797059837275\n",
      "  (0, 1563)\t0.20875340280264512\n",
      "  (0, 779)\t0.06839902359627349\n",
      "  (0, 1035)\t0.022943695992032588\n",
      "  (0, 29)\t0.0855003973638563\n",
      "  (0, 839)\t0.09392989548470095\n",
      "  (0, 1927)\t0.06579763264400111\n",
      "  (0, 1800)\t0.05910830179880021\n",
      "  (0, 877)\t0.07463940290723516\n",
      "  (0, 2039)\t0.10261940600890049\n",
      "  (0, 1276)\t0.05406144464962305\n",
      "  (0, 1581)\t0.20875340280264512\n",
      "  (0, 1160)\t0.10128618793379258\n",
      "  (0, 2004)\t0.037858496447795785\n",
      "  (0, 2041)\t0.050240098331598336\n",
      "  (0, 1094)\t0.1230251272349178\n",
      "  (0, 1838)\t0.10088053507516663\n",
      "  :\t:\n",
      "  (2033, 640)\t0.07289629705757049\n",
      "  (2033, 1608)\t0.07441608911470858\n",
      "  (2033, 704)\t0.09115293495944315\n",
      "  (2033, 1467)\t0.103291655405789\n",
      "  (2033, 1904)\t0.05678318136629984\n",
      "  (2033, 1216)\t0.11551544351314319\n",
      "  (2033, 344)\t0.07312893041900541\n",
      "  (2033, 2030)\t0.06525458668229758\n",
      "  (2033, 1843)\t0.10295688651054703\n",
      "  (2033, 236)\t0.0757005700665785\n",
      "  (2033, 239)\t0.09115293495944315\n",
      "  (2033, 1457)\t0.09931426656178584\n",
      "  (2033, 489)\t0.17906768670709464\n",
      "  (2033, 1413)\t0.09646107771593261\n",
      "  (2033, 683)\t0.18344444016128594\n",
      "  (2033, 1403)\t0.10865944426899014\n",
      "  (2033, 932)\t0.11795168326138407\n",
      "  (2033, 1848)\t0.09574697722554738\n",
      "  (2033, 51)\t0.3465355759639142\n",
      "  (2033, 1429)\t0.10075456233123241\n",
      "  (2033, 928)\t0.08343880863390854\n",
      "  (2033, 970)\t0.0998783520786057\n",
      "  (2033, 1376)\t0.0929103093115153\n",
      "  (2033, 1095)\t0.35370706985713035\n",
      "  (2033, 1559)\t0.11414875077652492\n",
      "(2034, 2048) (2034,)\n",
      "(1353, 2048) (1353,)\n"
     ]
    }
   ],
   "source": [
    "num_train = len(newsgroups_train.data)\n",
    "num_test  = len(newsgroups_test.data)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2048)    #文本特征提取\n",
    "\n",
    "data = vectorizer.fit_transform( newsgroups_train.data + newsgroups_test.data )\n",
    "X_train = data[0:num_train, :]\n",
    "X_test = data[num_train:num_train+num_test,:]\n",
    "\n",
    "Y_train = newsgroups_train.target\n",
    "Y_test = newsgroups_test.target\n",
    "\n",
    "print(X_train)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z1 = X * W1 + b1  \n",
    "a1 = ReLU(z1)  \n",
    "z2 = a1 * W2 + b2  \n",
    "a2 = tanh(z2)  \n",
    "z3 = a2 * W3 + b3  \n",
    "a3 = sigmoid(z3)  \n",
    "z4 = a3 * W4 + b4  \n",
    "a4 = $\\hat{y}$ = softmax(z4)  \n",
    "![avater](./loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate the total loss on the dataset\n",
    "def calculate_loss(model, X, y):\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3'], model['W4'], model['b4']\n",
    "    #正向传播，计算预测值\n",
    "    z1 = X.dot(W1) + b1\n",
    "    a1 = np.maximum(0, z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = np.tanh(z2)\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    a3 = 1./(1 + np.exp(-z3))\n",
    "    z4 = a3.dot(W4) + b4\n",
    "    exp_scores = np.exp(z4)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    # 计算损失\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    #在损失上加上正则项（可选）\n",
    "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)) + np.sum(np.square(W4)))\n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, X):\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'], model['b3'], model['W4'], model['b4']\n",
    "    #正向传播，计算预测值\n",
    "    z1 = X.dot(W1) + b1\n",
    "    a1 = np.maximum(0, z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = np.tanh(z2)\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    a3 = 1./(1 + np.exp(-z3))\n",
    "    z4 = a3.dot(W4) + b4\n",
    "    exp_scores = np.exp(z4)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, level):\n",
    "    if level < 0 or level >= 1:\n",
    "        print(\"Dropout level must be in interval [0,1)\")\n",
    "    retain_prob = 1 - level\n",
    "    sample = np.random.binomial(1, retain_prob, np.shape(X))  #生成一个0，1分布的向量\n",
    "    #print('X_shape',np.shape(X), type(X))\n",
    "    #print('sample',np.shape(sample), type(sample))\n",
    "    X = X * sample\n",
    "    X /= retain_prob\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function learns parameters for the neural network and returns the model.\n",
    "# - nn_hdim: Number of nodes in the hidden layer\n",
    "# - num_passes: Number of passes through the training data for gradient descent\n",
    "# - print_loss: If True, print the loss every 1000 iterations\n",
    "def build_model(X, y, nn_hdim, epsilon, reg_lambda, num_passes=20000,  print_loss=False):\n",
    "# 用随机值初始化参数。我们需要学习这些参数\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(input_dim, nn_hdim[0]) / np.sqrt(input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim[0]))\n",
    "    W2 = np.random.randn(nn_hdim[0],nn_hdim[1]) / np.sqrt(nn_hdim[0])\n",
    "    b2 = np.zeros((1, nn_hdim[1]))\n",
    "    W3 = np.random.randn(nn_hdim[1],nn_hdim[2]) / np.sqrt(nn_hdim[1])\n",
    "    b3 = np.zeros((1, nn_hdim[2]))\n",
    "    W4 = np.random.randn(nn_hdim[2], np.shape(categories)[0]) / np.sqrt(nn_hdim[2])\n",
    "    b4 = np.zeros((1, np.shape(categories)[0]))\n",
    "\n",
    "    # 这是我们最终要返回的数据\n",
    "    model = {}\n",
    "\n",
    "    # 梯度下降\n",
    "    for i in range(0, num_passes):\n",
    "        #print(i)\n",
    "        #正向传播，计算预测值\n",
    "        #Y = dropout(X, 0.2)\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = np.maximum(0, z1)\n",
    "        a1 = dropout(a1, 0.01)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        a2 = np.tanh(z2)\n",
    "        a2 = dropout(a2, 0)\n",
    "        z3 = a2.dot(W3) + b3\n",
    "        a3 = 1./(1 + np.exp(-z3))\n",
    "        a3 = dropout(a3, 0)\n",
    "        z4 = a3.dot(W4) + b4\n",
    "        exp_scores = np.exp(z4)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        # 反向传播\n",
    "        delta4 = probs\n",
    "        delta4[range(num_examples), y] -= 1\n",
    "        dW4 = (a3.T).dot(delta4)\n",
    "        db4 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        delta3 = delta4.dot(W4.T) * a3 * (1 - a3)\n",
    "        dW3 = (a2.T).dot(delta3)\n",
    "        db3 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W3.T) * (1 - np.power(a2, 2))\n",
    "        dW2 = (a1.T).dot(delta2)\n",
    "        db2 = np.sum(delta2, axis=0)\n",
    "        delta1 = delta2.dot(W2.T)\n",
    "        delta1[z1 <= 0] = 0\n",
    "        dW1 = (X.T).dot(delta1)\n",
    "        db1 = np.sum(delta1, axis=0)\n",
    "        \n",
    "\n",
    "\n",
    "        # 添加正则项 (b1 和 b2 没有正则项)\n",
    "        dW4 += reg_lambda * W4\n",
    "        dW3 += reg_lambda * W3\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "        \n",
    "        #print(dW1)\n",
    "        #print(dW2)\n",
    "        #print(dW3)\n",
    "        #print(dW4)\n",
    "\n",
    "        # 梯度下降更新参数\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        W3 += -epsilon * dW3\n",
    "        b3 += -epsilon * db3\n",
    "        W4 += -epsilon * dW4\n",
    "        b4 += -epsilon * db4\n",
    "        \n",
    "        \n",
    "        # 为模型分配新的参数\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3, 'W4': W4, 'b4': b4 }\n",
    "        #print(model)\n",
    "        #yp = predict(model, X_test)\n",
    "        #print(yp)\n",
    "        #print('Accuracy %f'%(np.mean(yp==Y_test)))\n",
    "\n",
    "        # 选择性地打印损失\n",
    "        # 这种做法很奢侈，因为我们用的是整个数据集，所以我们不想太频繁地这样做\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print (\"Loss after iteration %i: %f\" %(i, calculate_loss(model,X,y)))\n",
    "            epsilon = epsilon * 0.95\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.387040\n",
      "Loss after iteration 1000: 0.624583\n",
      "Loss after iteration 2000: 0.146109\n",
      "Loss after iteration 3000: 0.023992\n",
      "Loss after iteration 4000: 0.014799\n",
      "Loss after iteration 5000: 0.011928\n",
      "Loss after iteration 6000: 0.010175\n",
      "Loss after iteration 7000: 0.008622\n",
      "Loss after iteration 8000: 0.007645\n",
      "Loss after iteration 9000: 0.007020\n",
      "Loss after iteration 10000: 0.006623\n",
      "Loss after iteration 11000: 0.006395\n",
      "Loss after iteration 12000: 0.006305\n",
      "Loss after iteration 13000: 0.006125\n",
      "Loss after iteration 14000: 0.005854\n",
      "Loss after iteration 15000: 0.005835\n",
      "Loss after iteration 16000: 0.005720\n",
      "Loss after iteration 17000: 0.005735\n",
      "Loss after iteration 18000: 0.005651\n",
      "Loss after iteration 19000: 0.005559\n",
      "Loss after iteration 20000: 0.005425\n",
      "Loss after iteration 21000: 0.005493\n",
      "Loss after iteration 22000: 0.005382\n",
      "Loss after iteration 23000: 0.005385\n",
      "Loss after iteration 24000: 0.005331\n",
      "Loss after iteration 25000: 0.005234\n",
      "Loss after iteration 26000: 0.005127\n",
      "Loss after iteration 27000: 0.005145\n",
      "Loss after iteration 28000: 0.005099\n",
      "Loss after iteration 29000: 0.005019\n",
      "Loss after iteration 30000: 0.004977\n",
      "Loss after iteration 31000: 0.004906\n",
      "Loss after iteration 32000: 0.004825\n",
      "Loss after iteration 33000: 0.004821\n",
      "Loss after iteration 34000: 0.004731\n",
      "Loss after iteration 35000: 0.004681\n",
      "Loss after iteration 36000: 0.004597\n",
      "Loss after iteration 37000: 0.004515\n",
      "Loss after iteration 38000: 0.004523\n",
      "Loss after iteration 39000: 0.004440\n",
      "Loss after iteration 40000: 0.004375\n",
      "Loss after iteration 41000: 0.004327\n",
      "Loss after iteration 42000: 0.004282\n",
      "Loss after iteration 43000: 0.004234\n",
      "Loss after iteration 44000: 0.004188\n",
      "Loss after iteration 45000: 0.004182\n",
      "Loss after iteration 46000: 0.004096\n",
      "Loss after iteration 47000: 0.004095\n",
      "Loss after iteration 48000: 0.004033\n",
      "Loss after iteration 49000: 0.004005\n",
      "Loss after iteration 50000: 0.003984\n",
      "Loss after iteration 51000: 0.003955\n",
      "Loss after iteration 52000: 0.003929\n",
      "Loss after iteration 53000: 0.003921\n",
      "Loss after iteration 54000: 0.003906\n",
      "Loss after iteration 55000: 0.003869\n",
      "Loss after iteration 56000: 0.003842\n",
      "Loss after iteration 57000: 0.003832\n",
      "Loss after iteration 58000: 0.003811\n",
      "Loss after iteration 59000: 0.003801\n",
      "Loss after iteration 60000: 0.003771\n",
      "Loss after iteration 61000: 0.003760\n",
      "Loss after iteration 62000: 0.003732\n",
      "Loss after iteration 63000: 0.003734\n",
      "Loss after iteration 64000: 0.003717\n",
      "Loss after iteration 65000: 0.003696\n",
      "Loss after iteration 66000: 0.003691\n",
      "Loss after iteration 67000: 0.003689\n",
      "Loss after iteration 68000: 0.003674\n",
      "Loss after iteration 69000: 0.003666\n",
      "Loss after iteration 70000: 0.003651\n",
      "Loss after iteration 71000: 0.003651\n",
      "Loss after iteration 72000: 0.003640\n",
      "Loss after iteration 73000: 0.003637\n",
      "Loss after iteration 74000: 0.003619\n",
      "Loss after iteration 75000: 0.003616\n",
      "Loss after iteration 76000: 0.003599\n",
      "Loss after iteration 77000: 0.003607\n",
      "Loss after iteration 78000: 0.003601\n",
      "Loss after iteration 79000: 0.003593\n",
      "Loss after iteration 80000: 0.003587\n",
      "Loss after iteration 81000: 0.003583\n",
      "Loss after iteration 82000: 0.003576\n",
      "Loss after iteration 83000: 0.003578\n",
      "Loss after iteration 84000: 0.003568\n",
      "Loss after iteration 85000: 0.003563\n",
      "Loss after iteration 86000: 0.003560\n",
      "Loss after iteration 87000: 0.003558\n",
      "Loss after iteration 88000: 0.003556\n",
      "Loss after iteration 89000: 0.003554\n",
      "Loss after iteration 90000: 0.003547\n",
      "Loss after iteration 91000: 0.003543\n",
      "Loss after iteration 92000: 0.003540\n",
      "Loss after iteration 93000: 0.003537\n",
      "Loss after iteration 94000: 0.003532\n",
      "Loss after iteration 95000: 0.003534\n",
      "Loss after iteration 96000: 0.003531\n",
      "Loss after iteration 97000: 0.003530\n",
      "Loss after iteration 98000: 0.003531\n",
      "Loss after iteration 99000: 0.003527\n"
     ]
    }
   ],
   "source": [
    "# Build a model with a 3-dimensional hidden layer\n",
    "\n",
    "num_examples, input_dim = X_train.shape\n",
    "#print(input_dim)\n",
    "epsilon = 0.0001\n",
    "reg_lambda = 0.01\n",
    "epochs = 100000\n",
    "nn_hdim = [4,16,8]\n",
    "\n",
    "model = build_model(X_train, Y_train, nn_hdim, epsilon, reg_lambda, epochs, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.825573 = 1117 / 1353\n"
     ]
    }
   ],
   "source": [
    "n_correct = 0\n",
    "n_test = X_test.shape[0]\n",
    "for n in range(n_test):\n",
    "    x = X_test[n,:]\n",
    "    yp = predict(model, x)\n",
    "    if yp == Y_test[n]:\n",
    "        n_correct += 1.0\n",
    "\n",
    "print('Accuracy %f = %d / %d'%(n_correct/n_test, int(n_correct), n_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
