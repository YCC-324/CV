{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#Load the modules\nfrom glob import glob \nimport numpy as np\nimport pandas as pd\nimport keras,cv2,os\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\nfrom keras.layers import Conv2D, MaxPool2D\n\nfrom tqdm import tqdm_notebook,trange\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport random\nfrom sklearn.utils import shuffle\nfrom tqdm import tqdm_notebook\n\nimport gc #garbage collection, we need to save all the RAM we can\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n#\n\n# Any results you write to the current directory are saved as output.\nb=os.listdir(\"../input\")\nprint(b)","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"['test', 'train', 'sample_submission.csv', 'train_labels.csv']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Load the labels（0表示没有癌细胞（负样本），1表示有（正样本）） and filenames"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#set paths to training and test data\npath = \"../input/\" #adapt this path, when running locally\ntrain_path = path + 'train/'\ntest_path = path + 'test/'\n\ndf = pd.DataFrame({'path': glob(os.path.join(train_path,'*.tif'))}) # load the filenames\ndf['id'] = df.path.map(lambda x: x.split('/')[3].split(\".\")[0]) # keep only the file names in 'id'\nlabels = pd.read_csv(path+\"train_labels.csv\") # read the provided labels\ndf = df.merge(labels, on = \"id\") # merge labels and filepaths\ndf.head(3) # print the first three entrys","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"                                                path  ...  label\n0  ../input/train/53cd84dd8190c3b61869e5c6e364ef7...  ...      0\n1  ../input/train/1fd38079166dec3f0b2d10111c1326b...  ...      0\n2  ../input/train/698888e4039fcc82bec0414a5a1c452...  ...      0\n\n[3 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../input/train/53cd84dd8190c3b61869e5c6e364ef7...</td>\n      <td>53cd84dd8190c3b61869e5c6e364ef71618d0803</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../input/train/1fd38079166dec3f0b2d10111c1326b...</td>\n      <td>1fd38079166dec3f0b2d10111c1326beaa71ab7b</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../input/train/698888e4039fcc82bec0414a5a1c452...</td>\n      <td>698888e4039fcc82bec0414a5a1c452673a075de</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Load the images"},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we count the statistics, we can check if there are any completely black or white images\ndark_th = 10      # If no pixel reaches this threshold,image is considered too dark\nbright_th = 245   # If no pixel is under this threshold,image is considerd too bright\ntoo_dark_idx = []\ntoo_bright_idx = []\n\nORIGINAL_SIZE = 96      # original size of the images - do not change\n# AUGMENTATION VARIABLES\nCROP_SIZE = 90          # final size after crop\nRANDOM_ROTATION = 3    # range (0-180), 180 allows all rotation variations, 0=no change\nRANDOM_SHIFT = 2        # center crop shift in x and y axes, 0=no change. This cannot be more than (ORIGINAL_SIZE - CROP_SIZE)//2 \nRANDOM_BRIGHTNESS = 7  # range (0-100), 0=no change\nRANDOM_CONTRAST = 5    # range (0-100), 0=no change\nRANDOM_90_DEG_TURN = 1  # 0 or 1= random turn to left or right \n\ndef load_data(N,df):\n    \"\"\" This functions loads N images using the data df\n    \"\"\"\n    # allocate a numpy array for the images (N, 96x96px, 3 channels, values 0 - 255)\n    X = np.zeros([N,96,96,3],dtype=np.float32) \n    y = np.zeros([N],dtype=np.float32) \n    #convert the labels to a numpy array too\n    #read images one by one, tdqm notebook displays a progress bar\n    for i, row in tqdm_notebook(df.iterrows(), total=N):\n        if i == N:\n            break\n        bgr_img = cv2.imread(row['path'])\n        rgb_img = bgr_img[:,:,::-1] \n        \n        '''\n       #random rotation\n        rotation = random.randint(-RANDOM_ROTATION,RANDOM_ROTATION)\n        if(RANDOM_90_DEG_TURN == 1):\n            rotation += random.randint(-1,1) * 90\n        M = cv2.getRotationMatrix2D((48,48),rotation,1)   # the center point is the rotation anchor\n        rgb_img = cv2.warpAffine(rgb_img,M,(96,96)) \n        \n        '''\n        '''\n        # Random flip\n        flip_hor = bool(random.getrandbits(1))\n        flip_ver = bool(random.getrandbits(1))\n        if(flip_hor):\n            rgb_img = rgb_img[:, ::-1]\n        if(flip_ver):\n            rgb_img = rgb_img[::-1, :]\n        '''    \n        \n        # Random brightness\n        #br = random.randint(-RANDOM_BRIGHTNESS, RANDOM_BRIGHTNESS) / 100.\n        #rgb_img = rgb_img + br\n        '''\n        # Random contrast\n        cr = 1.0 + random.randint(-RANDOM_CONTRAST, RANDOM_CONTRAST) / 100.\n        rgb_img = rgb_img * cr\n        #print(rgb_img.max())\n        #print(rgb_img.min())\n        #print('\\n')\n        # is this too dark\n        \n        '''   \n                \n        ''' \n        if(rgb_img.max() < dark_th):\n            too_dark_idx.append(i)\n            continue # do not include in statistics\n        # is this too bright\n        if(rgb_img.min() > bright_th):\n            too_bright_idx.append(i)\n            \n            continue # do not include in statistics\n        '''\n        X[i] = rgb_img\n        y[i] = np.squeeze(df.as_matrix(columns=['label']))[i]\n      \n    return X,y","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"loading all the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 10000 #df[\"path\"].size # get the number of images in the training data set\nX,y = load_data(N=N,df=df)","execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8b74a6d14e6435997619c32bbaf4cdf"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:74: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"free up space in our RAM"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Collect garbage\npositives_samples = None\nnegative_samples = None\ntoo_dark_idx =None\ntoo_bright_idx =None\ngc.collect();","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"compile  \n配置模型训练时的参数"},{"metadata":{"trusted":true},"cell_type":"code","source":"def printplot(time,accurate):\n    i=os.system(\"cls\")\n    plt.plot(range(time), accurate, mec='r', mfc='w',label=u'曲线图')\n    plt.legend()  # 让图例生效\n    plt.xlabel(u\"time(s)邻居\") #X轴标签\n    plt.ylabel(\"RMSE\") #Y轴标签\n    plt.title(\"A simple plot\") #标题\n\n    plt.show()","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"batch size是训练一个神经网络是很关键的参数。在训练的时候，我们将训练集分成一个个的batch，然后用batch一个个的去训练我们的网络"},{"metadata":{},"cell_type":"markdown","source":"Train and validate the model"},{"metadata":{},"cell_type":"markdown","source":"Create a submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import division\n\nimport six\nfrom keras.models import Model\nfrom keras.layers import (\n    Input,\n    Activation,\n    Dense,\n    Flatten\n)\nfrom keras.layers.convolutional import (\n    Conv2D,\n    MaxPooling2D,\n    AveragePooling2D\n)\nfrom keras.layers.merge import add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.regularizers import l2\nfrom keras import backend as K\n\n\ndef _bn_relu(input):\n    \"\"\"Helper to build a BN -> relu block\n    \"\"\"\n    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n    return Activation(\"relu\")(norm)\n\n\ndef _conv_bn_relu(**conv_params):\n    \"\"\"Helper to build a conv -> BN -> relu block\n    \"\"\"\n    filters = conv_params[\"filters\"]\n    kernel_size = conv_params[\"kernel_size\"]\n    strides = conv_params.setdefault(\"strides\", (1, 1))\n    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n    padding = conv_params.setdefault(\"padding\", \"same\")\n    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n\n    def f(input):\n        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n                      strides=strides, padding=padding,\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer)(input)\n        return _bn_relu(conv)\n\n    return f\n\n\ndef _bn_relu_conv(**conv_params):\n    \"\"\"Helper to build a BN -> relu -> conv block.\n    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n    \"\"\"\n    filters = conv_params[\"filters\"]\n    kernel_size = conv_params[\"kernel_size\"]\n    strides = conv_params.setdefault(\"strides\", (1, 1))\n    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n    padding = conv_params.setdefault(\"padding\", \"same\")\n    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n\n    def f(input):\n        activation = _bn_relu(input)\n        return Conv2D(filters=filters, kernel_size=kernel_size,\n                      strides=strides, padding=padding,\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer)(activation)\n\n    return f\n\n\ndef _shortcut(input, residual):\n    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n    \"\"\"\n    # Expand channels of shortcut to match residual.\n    # Stride appropriately to match residual (width, height)\n    # Should be int if network architecture is correctly configured.\n    input_shape = K.int_shape(input)\n    residual_shape = K.int_shape(residual)\n    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n\n    shortcut = input\n    # 1 X 1 conv if shape is different. Else identity.\n    if stride_width > 1 or stride_height > 1 or not equal_channels:\n        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n                          kernel_size=(1, 1),\n                          strides=(stride_width, stride_height),\n                          padding=\"valid\",\n                          kernel_initializer=\"he_normal\",\n                          kernel_regularizer=l2(0.0001))(input)\n\n    return add([shortcut, residual])\n\n\ndef _residual_block(block_function, filters, repetitions, is_first_layer=False):\n    \"\"\"Builds a residual block with repeating bottleneck blocks.\n    \"\"\"\n    def f(input):\n        for i in range(repetitions):\n            init_strides = (1, 1)\n            if i == 0 and not is_first_layer:\n                init_strides = (2, 2)\n            input = block_function(filters=filters, init_strides=init_strides,\n                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n        return input\n\n    return f\n\n\ndef basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n    \"\"\"\n    def f(input):\n\n        if is_first_block_of_first_layer:\n            # don't repeat bn->relu since we just did bn->relu->maxpool\n            conv1 = Conv2D(filters=filters, kernel_size=(3, 3), dilation_rate = (2,2),\n                           strides=init_strides,\n                           padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=l2(1e-4))(input)\n        else:\n            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),dilation_rate = (2,2),\n                                  strides=init_strides)(input)\n\n        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n        return _shortcut(input, residual)\n\n    return f\n\n\ndef bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n    \"\"\"Bottleneck architecture for > 34 layer resnet.\n    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n    Returns:\n        A final conv layer of filters * 4\n    \"\"\"\n    def f(input):\n\n        if is_first_block_of_first_layer:\n            # don't repeat bn->relu since we just did bn->relu->maxpool\n            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),dilation_rate = (2,2),\n                              strides=init_strides,\n                              padding=\"same\",\n                              kernel_initializer=\"he_normal\",\n                              kernel_regularizer=l2(1e-4))(input)\n        else:\n            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),dilation_rate = (2,2),\n                                     strides=init_strides)(input)\n\n        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n        return _shortcut(input, residual)\n\n    return f\n\n\ndef _handle_dim_ordering():\n    global ROW_AXIS\n    global COL_AXIS\n    global CHANNEL_AXIS\n    if K.image_dim_ordering() == 'tf':\n        ROW_AXIS = 1\n        COL_AXIS = 2\n        CHANNEL_AXIS = 3\n    else:\n        CHANNEL_AXIS = 1\n        ROW_AXIS = 2\n        COL_AXIS = 3\n\n\ndef _get_block(identifier):\n    if isinstance(identifier, six.string_types):\n        res = globals().get(identifier)\n        if not res:\n            raise ValueError('Invalid {}'.format(identifier))\n        return res\n    return identifier\n\n\nclass ResnetBuilder(object):\n    @staticmethod\n    def build(input_shape, num_outputs, block_fn, repetitions):\n        \"\"\"Builds a custom ResNet like architecture.\n        Args:\n            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n            num_outputs: The number of outputs at final softmax layer\n            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n                The original paper used basic_block for layers < 50\n            repetitions: Number of repetitions of various block units.\n                At each block unit, the number of filters are doubled and the input size is halved\n        Returns:\n            The keras `Model`.\n        \"\"\"\n        _handle_dim_ordering()\n        if len(input_shape) != 3:\n            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n\n        # Permute dimension order if necessary\n        if K.image_dim_ordering() == 'tf':\n            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n\n        # Load function from str if needed.\n        block_fn = _get_block(block_fn)\n\n        input = Input(shape=input_shape)\n        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n        #pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n\n        #block = pool1\n        block = conv1\n        filters = 64\n        for i, r in enumerate(repetitions):\n            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n            filters *= 2\n\n        # Last activation\n        block = _bn_relu(block)\n\n        # Classifier block\n        block_shape = K.int_shape(block)\n        #pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n        #                         strides=(1, 1))(block)\n        #flatten1 = Flatten()(pool2)\n        flatten1 = Flatten()(block)\n        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n                      activation=\"softmax\")(flatten1)\n\n        model = Model(inputs=input, outputs=dense)\n        return model\n\n    @staticmethod\n    def build_resnet_18(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n\n    @staticmethod\n    def build_resnet_34(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n\n    @staticmethod\n    def build_resnet_50(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n\n    @staticmethod\n    def build_resnet_101(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n\n    @staticmethod\n    def build_resnet_152(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import np_utils\nfrom keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n\n\nlr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\nearly_stopper = EarlyStopping(min_delta=0.001, patience=10)\ncsv_logger = CSVLogger('resnet18_cifar10.csv')\n\nbatch_size = 50\nnb_classes = 2\nnb_epoch = 200\ndata_augmentation = True\n\n# input image dimensions\nimg_rows, img_cols = 96, 96\n# The CIFAR10 images are RGB.\nimg_channels = 3\nsplit_idx=8000\nN=10000\n# The data, shuffled and split between train and test sets:\nX_train=X[0:split_idx]\ny_train=y[0:split_idx]\nX_test =X[split_idx+1:N]\ny_test =y[split_idx+1:N]\n\n#del X,y\n\n# Convert class vectors to binary class matrices.\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\n\nX_train= X_train.astype('float32')\nX_test = X_test.astype('float32')\ngc.collect()\n\n# subtract mean and normalize\nmean_image = np.mean(X_train, axis=0)\nX_train -= mean_image\nX_test -= mean_image\nX_train /= 128.\nX_test /= 128.\n\nmodel = ResnetBuilder.build_resnet_50((img_channels, img_rows, img_cols), nb_classes)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nif not data_augmentation:\n    print('Not using data augmentation.')\n    model.fit(X_train, Y_train,\n              batch_size=batch_size,\n              nb_epoch=nb_epoch,\n              validation_data=(X[split_idx+1:N], y[split_idx+1:N]),\n              shuffle=True,\n              callbacks=[lr_reducer, early_stopper, csv_logger])\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for featurewise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    #datagen.fit(X_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    history=model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n                        steps_per_epoch=X_train.shape[0] // batch_size,\n                        validation_data=(X_train,Y_train),\n                        epochs=nb_epoch, verbose=1, max_q_size=100,\n                        callbacks=[lr_reducer, early_stopper, csv_logger])\n    ","execution_count":null,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nUsing real-time data augmentation.\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:84: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=160, validation_data=(array([[[..., epochs=200, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n","name":"stderr"},{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/200\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history.history.keys())\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('resnet 50 loss rmsprop')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.savefig('loss.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}